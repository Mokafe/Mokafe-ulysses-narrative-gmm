{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0378920d",
   "metadata": {},
   "source": [
    "# Ulysses narrative GMM: boundary (entropy) report + turning points Top-K\n",
    "\n",
    "このノートブックは、`ulysses_stream.csv`（特徴量CSV）と `Ulysses_fixed.json`（根拠文付きJSON）から、\n",
    "\n",
    "- GMMで潜在状態（K個）を推定\n",
    "- 各時点の posterior entropy を **boundaryness**（境界っぽさ）として算出\n",
    "- entropy 上位（ALPHA%）のスパンを **前後evidence付きでレポート化**\n",
    "- 本文・論文に貼れる **turning points TopK表** を生成\n",
    "\n",
    "を **単独でColab実行**できる形でまとめます。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa21da6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 0) Setup (Colab)\n",
    "# ============================================================\n",
    "# 基本はColab標準で動きます。もしsklearnが無い環境なら以下を実行してください。\n",
    "# !pip -q install scikit-learn pandas numpy\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Tuple, Any, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Colabで display を使うため\n",
    "from IPython.display import display\n",
    "\n",
    "RANDOM_SEED = 0\n",
    "np.random.seed(RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb020b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1) Config (ここだけ触ればOK)\n",
    "# ============================================================\n",
    "@dataclass\n",
    "class Config:\n",
    "    csv_path: str = \"/content/ulysses_stream.csv\"\n",
    "    json_path: str = \"/content/Ulysses_fixed.json\"\n",
    "\n",
    "    # Model\n",
    "    k: int = 8\n",
    "    covariance_type: str = \"diag\"     # \"diag\" 推奨（安定・軽量）\n",
    "    n_init: int = 10\n",
    "    max_iter: int = 500\n",
    "    reg_covar: float = 1e-6\n",
    "\n",
    "    # Boundary selection\n",
    "    alpha: float = 0.05               # entropy上位割合（0.05=上位5%）\n",
    "\n",
    "    # Output\n",
    "    out_dir: str = \"/content/out\"\n",
    "    turning_points_topk: int = 10\n",
    "    strong_margin_max: float = 0.10     # “強い境界”の margin 上限（例: 0.10）\n",
    "    strong_topk: int = 10               # 強い境界 TopK\n",
    "    transition_topk: int = 10           # 遷移点（state_change）TopK\n",
    "    shorten_text: bool = False\n",
    "    maxlen: int = 160                 # shorten_text=True のとき有効\n",
    "\n",
    "CFG = Config()\n",
    "\n",
    "print(CFG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfbcca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2) (Option) Upload your data files\n",
    "# ============================================================\n",
    "# - すでに /content に置いてあるなら、このセルは不要です。\n",
    "# - Colab左の「ファイル」→アップロードでもOKです。\n",
    "#\n",
    "# 使い方：\n",
    "# 1) 実行するとファイル選択ダイアログが出ます\n",
    "# 2) ulysses_stream.csv と Ulysses_fixed.json をアップロード\n",
    "# 3) CFG.csv_path / CFG.json_path が自動で更新されます\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "    uploaded = files.upload()\n",
    "    # アップロードされたファイル名から自動設定\n",
    "    for fn in uploaded.keys():\n",
    "        if fn.lower().endswith(\".csv\") and \"ulysses\" in fn.lower():\n",
    "            CFG.csv_path = f\"/content/{fn}\"\n",
    "        if fn.lower().endswith(\".json\") and \"ulysses\" in fn.lower():\n",
    "            CFG.json_path = f\"/content/{fn}\"\n",
    "    print(\"Updated paths:\", CFG.csv_path, CFG.json_path)\n",
    "except Exception as e:\n",
    "    print(\"Not running on Colab or upload skipped:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8d2dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3) Helpers\n",
    "# ============================================================\n",
    "def pick_evidence(row: pd.Series) -> str:\n",
    "    \"\"\"Prefer Japanese evidence if available, else fall back safely.\"\"\"\n",
    "    for col in [\"evidence_ja\", \"evidence_en\", \"evidence\", \"quote\", \"text\", \"raw_text\"]:\n",
    "        if col in row.index:\n",
    "            v = row[col]\n",
    "            if isinstance(v, str) and v.strip():\n",
    "                return v.strip()\n",
    "    return \"\"\n",
    "\n",
    "def pick_text(d: Dict[str, Any], keys: List[str]) -> str:\n",
    "    for k in keys:\n",
    "        v = d.get(k)\n",
    "        if isinstance(v, str) and v.strip():\n",
    "            return v.strip()\n",
    "    return \"\"\n",
    "\n",
    "def list_feature_cols(df: pd.DataFrame) -> List[str]:\n",
    "    prefixes = (\"mode_\", \"cause_\", \"place_\", \"myth_\", \"style_\")\n",
    "    cols = [c for c in df.columns if c.startswith(prefixes)]\n",
    "    if not cols:\n",
    "        raise ValueError(\"No feature columns found. Expected prefixes: mode_/cause_/place_/myth_/style_.\")\n",
    "    return cols\n",
    "\n",
    "def ensure_dir(p: str) -> Path:\n",
    "    path = Path(p)\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "    return path\n",
    "\n",
    "def flatten_json_spans(js: Any) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Support both: [chapter{time_series_data:[]}, ...] and {time_series_data:[]} and raw span list.\"\"\"\n",
    "    spans: List[Dict[str, Any]] = []\n",
    "    if isinstance(js, list):\n",
    "        for item in js:\n",
    "            if isinstance(item, dict) and \"time_series_data\" in item:\n",
    "                spans.extend(item.get(\"time_series_data\", []))\n",
    "            elif isinstance(item, dict):\n",
    "                spans.append(item)\n",
    "    elif isinstance(js, dict):\n",
    "        spans = js.get(\"time_series_data\", []) or []\n",
    "    return [s for s in spans if isinstance(s, dict)]\n",
    "\n",
    "def build_span_index(spans: List[Dict[str, Any]]) -> Dict[Tuple[int, int], Dict[str, Any]]:\n",
    "    \"\"\"Index by (episode, global_step).\"\"\"\n",
    "    idx: Dict[Tuple[int, int], Dict[str, Any]] = {}\n",
    "    for s in spans:\n",
    "        ep = s.get(\"episode\")\n",
    "        gs = s.get(\"global_step\")\n",
    "        if ep is None or gs is None:\n",
    "            continue\n",
    "        try:\n",
    "            idx[(int(ep), int(gs))] = s\n",
    "        except Exception:\n",
    "            continue\n",
    "    return idx\n",
    "\n",
    "def infer_keys(df: pd.DataFrame) -> Tuple[str, str]:\n",
    "    \"\"\"Return (episode_col, step_col) for CSV side.\"\"\"\n",
    "    # preferred\n",
    "    if \"episode\" in df.columns and \"global_step\" in df.columns:\n",
    "        return \"episode\", \"global_step\"\n",
    "    # common in your pipeline\n",
    "    if \"chapter\" in df.columns and \"span_id\" in df.columns:\n",
    "        return \"chapter\", \"span_id\"\n",
    "    # last resort\n",
    "    return \"\", \"\"\n",
    "\n",
    "def shorten(s: str, maxlen: int) -> str:\n",
    "    s = (s or \"\").replace(\"\\n\", \" \").strip()\n",
    "    return s if len(s) <= maxlen else s[:maxlen] + \"…\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4585ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4) Load CSV\n",
    "# ============================================================\n",
    "csv_path = Path(CFG.csv_path)\n",
    "if not csv_path.exists():\n",
    "    raise FileNotFoundError(f\"CSV not found: {csv_path}\")\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "feature_cols = list_feature_cols(df)\n",
    "X = df[feature_cols].to_numpy()\n",
    "\n",
    "episode_col, step_col = infer_keys(df)\n",
    "print(\"rows:\", len(df))\n",
    "print(\"feature_cols:\", len(feature_cols))\n",
    "print(\"csv keys:\", (episode_col, step_col))\n",
    "\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219a6de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5) Fit GMM + compute posterior entropy / margin\n",
    "# ============================================================\n",
    "Xz = StandardScaler().fit_transform(X)\n",
    "\n",
    "gmm = GaussianMixture(\n",
    "    n_components=CFG.k,\n",
    "    covariance_type=CFG.covariance_type,\n",
    "    random_state=RANDOM_SEED,\n",
    "    reg_covar=CFG.reg_covar,\n",
    "    max_iter=CFG.max_iter,\n",
    "    n_init=CFG.n_init\n",
    ")\n",
    "gmm.fit(Xz)\n",
    "\n",
    "resp = gmm.predict_proba(Xz)  # (N, K)\n",
    "entropy = -(resp * np.log(resp + 1e-12)).sum(axis=1)\n",
    "\n",
    "gmm_state = resp.argmax(axis=1)          # 0-based\n",
    "gmm_state_1based = gmm_state + 1         # 1-based\n",
    "state_change = np.r_[False, gmm_state[1:] != gmm_state[:-1]]\n",
    "\n",
    "# Top2 + margin\n",
    "order = np.argsort(-resp, axis=1)        # desc\n",
    "s1 = order[:, 0]\n",
    "s2 = order[:, 1]\n",
    "p1 = resp[np.arange(len(df)), s1]\n",
    "p2 = resp[np.arange(len(df)), s2]\n",
    "margin = p1 - p2\n",
    "\n",
    "print(\"entropy stats:\",\n",
    "      \"min=\", float(entropy.min()),\n",
    "      \"median=\", float(np.median(entropy)),\n",
    "      \"p95=\", float(np.quantile(entropy, 0.95)),\n",
    "      \"max=\", float(entropy.max()))\n",
    "\n",
    "print(\"state_change rate:\", float(state_change.mean()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716b6e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6) boundary candidates: entropy top ALPHA%\n",
    "# ============================================================\n",
    "if not (0 < CFG.alpha <= 1):\n",
    "    raise ValueError(\"alpha must be in (0, 1].\")\n",
    "\n",
    "n_top = int(np.ceil(len(df) * CFG.alpha))\n",
    "top_idx = np.argsort(entropy)[-n_top:][::-1]  # high entropy first\n",
    "\n",
    "print(\"ALPHA:\", CFG.alpha, \"=> n_top:\", n_top)\n",
    "print(\"top entropy:\", float(entropy[top_idx[0]]), \"at row\", int(top_idx[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2cc973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 7) Load JSON + index spans\n",
    "# ============================================================\n",
    "json_path = Path(CFG.json_path)\n",
    "if not json_path.exists():\n",
    "    raise FileNotFoundError(f\"JSON not found: {json_path}\")\n",
    "\n",
    "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    js = json.load(f)\n",
    "\n",
    "spans = flatten_json_spans(js)\n",
    "span_index = build_span_index(spans)\n",
    "\n",
    "print(\"json spans:\", len(spans))\n",
    "print(\"indexed spans:\", len(span_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1bb6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 8) Build boundary_report.csv (evidence prev/next + json span text)\n",
    "# ============================================================\n",
    "rows = []\n",
    "missing_json = 0\n",
    "\n",
    "for i in top_idx:\n",
    "    i = int(i)\n",
    "    # CSV keys\n",
    "    ep = int(df.loc[i, episode_col]) if episode_col else -1\n",
    "    gs = int(df.loc[i, step_col]) if step_col else i\n",
    "\n",
    "    tt = df.loc[i, \"transition_type\"] if \"transition_type\" in df.columns else \"\"\n",
    "\n",
    "    ev = pick_evidence(df.loc[i])\n",
    "    ev_prev = pick_evidence(df.loc[i - 1]) if i - 1 >= 0 else \"\"\n",
    "    ev_next = pick_evidence(df.loc[i + 1]) if i + 1 < len(df) else \"\"\n",
    "\n",
    "    s = span_index.get((ep, gs))\n",
    "    if s is None:\n",
    "        missing_json += 1\n",
    "        s = {}\n",
    "\n",
    "    row = {\n",
    "        \"row_index\": i,\n",
    "        \"episode\": ep,\n",
    "        \"global_step\": gs,\n",
    "        \"transition_type\": tt,\n",
    "\n",
    "        \"entropy\": float(entropy[i]),\n",
    "        \"gmm_state_1based\": int(gmm_state_1based[i]),\n",
    "        \"state_change\": bool(state_change[i]),\n",
    "\n",
    "        \"top1_state_1based\": int(s1[i] + 1),\n",
    "        \"top1_p\": float(p1[i]),\n",
    "        \"top2_state_1based\": int(s2[i] + 1),\n",
    "        \"top2_p\": float(p2[i]),\n",
    "        \"margin\": float(margin[i]),\n",
    "\n",
    "        \"evidence_prev\": ev_prev,\n",
    "        \"evidence\": ev,\n",
    "        \"evidence_next\": ev_next,\n",
    "\n",
    "        \"json_span_text_en\": pick_text(s, [\"span_text_en\", \"text_en\"]),\n",
    "        \"json_span_text_ja\": pick_text(s, [\"span_text_ja\", \"text_ja\"]),\n",
    "        \"json_evidence_en\": pick_text(s, [\"evidence_en\"]),\n",
    "        \"json_evidence_ja\": pick_text(s, [\"evidence_ja\"]),\n",
    "    }\n",
    "\n",
    "    rows.append(row)\n",
    "\n",
    "rep = pd.DataFrame(rows)\n",
    "\n",
    "if CFG.shorten_text:\n",
    "    for c in [\"evidence_prev\", \"evidence\", \"evidence_next\", \"json_span_text_en\", \"json_span_text_ja\"]:\n",
    "        if c in rep.columns:\n",
    "            rep[c] = rep[c].astype(str).map(lambda x: shorten(x, CFG.maxlen))\n",
    "\n",
    "out_dir = ensure_dir(CFG.out_dir)\n",
    "rep_path = out_dir / \"boundary_report.csv\"\n",
    "rep.to_csv(rep_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"saved:\", rep_path)\n",
    "print(\"json span missing for\", missing_json, \"rows (key mismatch or partial json).\")\n",
    "\n",
    "display(rep.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f95ccd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9) turning_points_topK.csv/tsv (paper-ready table)\n",
    "#    - sort by entropy desc, tie-break by margin asc\n",
    "# ============================================================\n",
    "tp = pd.DataFrame({\n",
    "    \"episode\": df[episode_col].astype(int) if episode_col else pd.Series([-1]*len(df)),\n",
    "    \"global_step\": df[step_col].astype(int) if step_col else pd.Series(np.arange(len(df))),\n",
    "\n",
    "    \"transition_type\": df[\"transition_type\"] if \"transition_type\" in df.columns else \"\",\n",
    "    \"entropy\": entropy,\n",
    "    \"gmm_state_1based\": (gmm_state + 1).astype(int),\n",
    "\n",
    "    \"top1_state_1based\": (s1 + 1).astype(int),\n",
    "    \"top1_p\": p1.astype(float),\n",
    "    \"top2_state_1based\": (s2 + 1).astype(int),\n",
    "    \"top2_p\": p2.astype(float),\n",
    "    \"margin\": margin.astype(float),\n",
    "\n",
    "    \"evidence_best\": df.apply(pick_evidence, axis=1),\n",
    "})\n",
    "\n",
    "tp = tp.sort_values([\"entropy\", \"margin\"], ascending=[False, True]).copy()\n",
    "\n",
    "tp_top = tp.head(CFG.turning_points_topk).copy()\n",
    "tp_top.insert(0, \"rank\", np.arange(1, len(tp_top) + 1))\n",
    "\n",
    "tp_csv = out_dir / \"turning_points_top10.csv\"\n",
    "tp_tsv = out_dir / \"turning_points_top10.tsv\"\n",
    "tp_top.to_csv(tp_csv, index=False, encoding=\"utf-8-sig\")\n",
    "tp_top.to_csv(tp_tsv, index=False, sep=\"\\t\", encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"saved:\", tp_csv)\n",
    "print(\"saved:\", tp_tsv)\n",
    "\n",
    "display(tp_top)\n",
    "\n",
    "print(\"sanity check (should be 1..K):\",\n",
    "      int(tp_top[\"gmm_state_1based\"].min()),\n",
    "      int(tp_top[\"gmm_state_1based\"].max()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fabd0f",
   "metadata": {},
   "source": [
    "## 追加1： “強い境界”だけ版（entropy上位 AND margin小）\n",
    "\n",
    "- boundaryness（entropy）が高く、かつ top1/top2 の差（margin）が小さい点は  \n",
    "  **「2状態で本当に割れている」**＝解釈しやすい境界になりやすいです。\n",
    "- ここでは **entropyが上位ALPHA%** かつ **margin <= strong_margin_max** を満たす点だけ抽出します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a94263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9A) Strong boundaries: (entropy top ALPHA%) AND (margin small)\n",
    "# ============================================================\n",
    "entropy_thr = float(np.quantile(entropy, 1.0 - CFG.alpha))\n",
    "strong_mask = (entropy >= entropy_thr) & (margin <= CFG.strong_margin_max)\n",
    "strong_idx = np.where(strong_mask)[0]\n",
    "\n",
    "print(\"entropy_thr (top ALPHA%):\", entropy_thr)\n",
    "print(\"strong_margin_max:\", CFG.strong_margin_max)\n",
    "print(\"strong boundary count:\", int(len(strong_idx)))\n",
    "\n",
    "# 0件になったら、現実的に使えるように「entropy降順・margin昇順で上位n_top」をフォールバック\n",
    "if len(strong_idx) == 0:\n",
    "    print(\"[fallback] no strong boundaries found -> take top n_top by (entropy desc, margin asc)\")\n",
    "    tmp = pd.DataFrame({\"i\": np.arange(len(df)), \"entropy\": entropy, \"margin\": margin})\n",
    "    tmp = tmp.sort_values([\"entropy\", \"margin\"], ascending=[False, True]).head(n_top)\n",
    "    strong_idx = tmp[\"i\"].astype(int).to_numpy()\n",
    "\n",
    "# Strong boundary report (same schema as boundary_report.csv)\n",
    "rows2 = []\n",
    "missing_json2 = 0\n",
    "for i in strong_idx:\n",
    "    i = int(i)\n",
    "    ep = int(df.loc[i, episode_col]) if episode_col else -1\n",
    "    gs = int(df.loc[i, step_col]) if step_col else i\n",
    "    tt = df.loc[i, \"transition_type\"] if \"transition_type\" in df.columns else \"\"\n",
    "\n",
    "    ev = pick_evidence(df.loc[i])\n",
    "    ev_prev = pick_evidence(df.loc[i - 1]) if i - 1 >= 0 else \"\"\n",
    "    ev_next = pick_evidence(df.loc[i + 1]) if i + 1 < len(df) else \"\"\n",
    "\n",
    "    s = span_index.get((ep, gs))\n",
    "    if s is None:\n",
    "        missing_json2 += 1\n",
    "        s = {}\n",
    "\n",
    "    rows2.append({\n",
    "        \"row_index\": i,\n",
    "        \"episode\": ep,\n",
    "        \"global_step\": gs,\n",
    "        \"transition_type\": tt,\n",
    "        \"entropy\": float(entropy[i]),\n",
    "        \"gmm_state_1based\": int(gmm_state_1based[i]),\n",
    "        \"state_change\": bool(state_change[i]),\n",
    "        \"top1_state_1based\": int(s1[i] + 1),\n",
    "        \"top1_p\": float(p1[i]),\n",
    "        \"top2_state_1based\": int(s2[i] + 1),\n",
    "        \"top2_p\": float(p2[i]),\n",
    "        \"margin\": float(margin[i]),\n",
    "        \"evidence_prev\": ev_prev,\n",
    "        \"evidence\": ev,\n",
    "        \"evidence_next\": ev_next,\n",
    "        \"json_span_text_en\": pick_text(s, [\"span_text_en\", \"text_en\"]),\n",
    "        \"json_span_text_ja\": pick_text(s, [\"span_text_ja\", \"text_ja\"]),\n",
    "        \"json_evidence_en\": pick_text(s, [\"evidence_en\"]),\n",
    "        \"json_evidence_ja\": pick_text(s, [\"evidence_ja\"]),\n",
    "    })\n",
    "\n",
    "strong_rep = pd.DataFrame(rows2)\n",
    "if CFG.shorten_text:\n",
    "    for c in [\"evidence_prev\", \"evidence\", \"evidence_next\", \"json_span_text_en\", \"json_span_text_ja\"]:\n",
    "        if c in strong_rep.columns:\n",
    "            strong_rep[c] = strong_rep[c].astype(str).map(lambda x: shorten(x, CFG.maxlen))\n",
    "\n",
    "strong_rep = strong_rep.sort_values([\"entropy\", \"margin\"], ascending=[False, True]).copy()\n",
    "\n",
    "strong_rep_path = out_dir / \"strong_boundary_report.csv\"\n",
    "strong_rep.to_csv(strong_rep_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"saved:\", strong_rep_path, \"| json missing:\", missing_json2)\n",
    "\n",
    "display(strong_rep.head(20))\n",
    "\n",
    "# Paper-ready TopK table for strong boundaries\n",
    "strong_tp = strong_rep.copy()\n",
    "strong_tp = strong_tp.rename(columns={\"evidence\": \"evidence_best\"})\n",
    "strong_tp = strong_tp[[\n",
    "    \"episode\",\"global_step\",\"transition_type\",\n",
    "    \"entropy\",\"gmm_state_1based\",\n",
    "    \"top1_state_1based\",\"top1_p\",\n",
    "    \"top2_state_1based\",\"top2_p\",\n",
    "    \"margin\",\"evidence_best\"\n",
    "]].copy()\n",
    "\n",
    "strong_tp = strong_tp.sort_values([\"entropy\",\"margin\"], ascending=[False, True]).head(CFG.strong_topk).copy()\n",
    "strong_tp.insert(0, \"rank\", np.arange(1, len(strong_tp) + 1))\n",
    "\n",
    "strong_tp_csv = out_dir / \"strong_turning_points_top10.csv\"\n",
    "strong_tp_tsv = out_dir / \"strong_turning_points_top10.tsv\"\n",
    "strong_tp.to_csv(strong_tp_csv, index=False, encoding=\"utf-8-sig\")\n",
    "strong_tp.to_csv(strong_tp_tsv, index=False, sep=\"\\t\", encoding=\"utf-8-sig\")\n",
    "print(\"saved:\", strong_tp_csv)\n",
    "print(\"saved:\", strong_tp_tsv)\n",
    "display(strong_tp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483c60c6",
   "metadata": {},
   "source": [
    "## 追加2： 遷移点（state_change）優先版（MAPが変わった点のみランキング）\n",
    "\n",
    "- `gmm_state = argmax(resp)` の **MAP状態が前後で変わった点**だけに絞ってランキングします。\n",
    "- ランキングは **entropy降順**、同点なら **margin昇順**（=より混合が強い点を優先）です。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d33274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9B) Transition points: state_change == True only\n",
    "# ============================================================\n",
    "trans_idx = np.where(state_change)[0]\n",
    "print(\"transition points (state_change=True):\", int(len(trans_idx)))\n",
    "\n",
    "trans_tp = tp.loc[trans_idx].copy()\n",
    "trans_tp = trans_tp.sort_values([\"entropy\", \"margin\"], ascending=[False, True]).head(CFG.transition_topk).copy()\n",
    "trans_tp.insert(0, \"rank\", np.arange(1, len(trans_tp) + 1))\n",
    "\n",
    "trans_csv = out_dir / \"transition_points_top10.csv\"\n",
    "trans_tsv = out_dir / \"transition_points_top10.tsv\"\n",
    "trans_tp.to_csv(trans_csv, index=False, encoding=\"utf-8-sig\")\n",
    "trans_tp.to_csv(trans_tsv, index=False, sep=\"\\t\", encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"saved:\", trans_csv)\n",
    "print(\"saved:\", trans_tsv)\n",
    "display(trans_tp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df82737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 10) Quick sanity summary (optional)\n",
    "# ============================================================\n",
    "summary = {\n",
    "    \"N\": int(len(df)),\n",
    "    \"K\": int(CFG.k),\n",
    "    \"alpha\": float(CFG.alpha),\n",
    "    \"boundary_n\": int(len(rep)),\n",
    "    \"entropy_max\": float(entropy.max()),\n",
    "    \"entropy_p95\": float(np.quantile(entropy, 0.95)),\n",
    "    \"entropy_median\": float(np.median(entropy)),\n",
    "    \"state_change_rate\": float(state_change.mean()),\n",
    "    \"boundary_state_change_rate\": float(rep[\"state_change\"].mean()),\n",
    "    \"json_missing_rate_in_boundary\": float(missing_json / max(1, len(rep))),\n",
    "}\n",
    "pd.DataFrame([summary])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a89567",
   "metadata": {},
   "source": [
    "## 出力物\n",
    "\n",
    "- `out/boundary_report.csv`  \n",
    "  entropy上位(ALPHA%)の候補。前後evidence、top2 states、margin、JSON側span_text付き。\n",
    "\n",
    "- `out/turning_points_top10.csv` / `out/turning_points_top10.tsv`  \n",
    "  本文に貼る用の TopK 表。\n",
    "\n",
    "## GitHubに上げるときのおすすめ\n",
    "\n",
    "- ノートブック名（例）: `01_ulysses_boundary_report.ipynb`\n",
    "- 依存を減らすため、外部自作モジュール無しで完結させています。\n",
    "- データファイルは `.gitignore` で除外し、READMEに「Colabでアップロードして動かす」手順を書くのが安全です。\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
