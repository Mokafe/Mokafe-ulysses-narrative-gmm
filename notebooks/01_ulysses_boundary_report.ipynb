{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0378920d",
      "metadata": {
        "id": "0378920d"
      },
      "source": [
        "# Ulysses narrative GMM: boundary (entropy) report + turning points Top-K\n",
        "\n",
        "このノートブックは、`ulysses_stream.csv`（特徴量CSV）と `Ulysses_fixed.json`（根拠文付きJSON）から、\n",
        "\n",
        "- GMMで潜在状態（K個）を推定\n",
        "- 各時点の posterior entropy を **boundaryness**（境界っぽさ）として算出\n",
        "- entropy 上位（ALPHA%）のスパンを **前後evidence付きでレポート化**\n",
        "- 本文・論文に貼れる **turning points TopK表** を生成\n",
        "\n",
        "を **単独でColab実行**できる形でまとめます。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "fa21da6d",
      "metadata": {
        "id": "fa21da6d"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 0) Setup (Colab)\n",
        "# ============================================================\n",
        "# 基本はColab標準で動きます。もしsklearnが無い環境なら以下を実行してください。\n",
        "# !pip -q install scikit-learn pandas numpy\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, Tuple, Any, List, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Colabで display を使うため\n",
        "from IPython.display import display\n",
        "\n",
        "RANDOM_SEED = 0\n",
        "np.random.seed(RANDOM_SEED)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "fb020b53",
      "metadata": {
        "id": "fb020b53",
        "outputId": "b013faec-c087-4802-b915-3dee32c48105",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Config(csv_path='/content/ulysses_stream.csv', json_path='/content/Ulysses_fixed.json', k=8, covariance_type='diag', n_init=10, max_iter=500, reg_covar=1e-06, alpha=0.05, out_dir='/content/out', turning_points_topk=10, strong_margin_max=0.1, strong_topk=10, transition_topk=10, shorten_text=False, maxlen=160)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 1) Config (ここだけ触ればOK)\n",
        "# ============================================================\n",
        "@dataclass\n",
        "class Config:\n",
        "    csv_path: str = \"/content/ulysses_stream.csv\"\n",
        "    json_path: str = \"/content/Ulysses_fixed.json\"\n",
        "\n",
        "    # Model\n",
        "    k: int = 8\n",
        "    covariance_type: str = \"diag\"     # \"diag\" 推奨（安定・軽量）\n",
        "    n_init: int = 10\n",
        "    max_iter: int = 500\n",
        "    reg_covar: float = 1e-6\n",
        "\n",
        "    # Boundary selection\n",
        "    alpha: float = 0.05               # entropy上位割合（0.05=上位5%）\n",
        "\n",
        "    # Output\n",
        "    out_dir: str = \"/content/out\"\n",
        "    turning_points_topk: int = 10\n",
        "    strong_margin_max: float = 0.10     # “強い境界”の margin 上限（例: 0.10）\n",
        "    strong_topk: int = 10               # 強い境界 TopK\n",
        "    transition_topk: int = 10           # 遷移点（state_change）TopK\n",
        "    shorten_text: bool = False\n",
        "    maxlen: int = 160                 # shorten_text=True のとき有効\n",
        "\n",
        "CFG = Config()\n",
        "\n",
        "print(CFG)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cfbcca8",
      "metadata": {
        "id": "7cfbcca8",
        "outputId": "0cc1daaf-15e1-4037-e43a-72edfa51ab8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 40
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-42e03a0e-727d-4a01-bc89-3c8cb70ec5dd\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-42e03a0e-727d-4a01-bc89-3c8cb70ec5dd\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 2) (Option) Upload your data files\n",
        "# ============================================================\n",
        "# - すでに /content に置いてあるなら、このセルは不要です。\n",
        "# - Colab左の「ファイル」→アップロードでもOKです。\n",
        "#\n",
        "# 使い方：\n",
        "# 1) 実行するとファイル選択ダイアログが出ます\n",
        "# 2) ulysses_stream.csv と Ulysses_fixed.json をアップロード\n",
        "# 3) CFG.csv_path / CFG.json_path が自動で更新されます\n",
        "\n",
        "try:\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()\n",
        "    # アップロードされたファイル名から自動設定\n",
        "    for fn in uploaded.keys():\n",
        "        if fn.lower().endswith(\".csv\") and \"ulysses\" in fn.lower():\n",
        "            CFG.csv_path = f\"/content/{fn}\"\n",
        "        if fn.lower().endswith(\".json\") and \"ulysses\" in fn.lower():\n",
        "            CFG.json_path = f\"/content/{fn}\"\n",
        "    print(\"Updated paths:\", CFG.csv_path, CFG.json_path)\n",
        "except Exception as e:\n",
        "    print(\"Not running on Colab or upload skipped:\", e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b8d2dca",
      "metadata": {
        "id": "0b8d2dca"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 3) Helpers\n",
        "# ============================================================\n",
        "def pick_evidence(row: pd.Series) -> str:\n",
        "    \"\"\"Prefer Japanese evidence if available, else fall back safely.\"\"\"\n",
        "    for col in [\"evidence_ja\", \"evidence_en\", \"evidence\", \"quote\", \"text\", \"raw_text\"]:\n",
        "        if col in row.index:\n",
        "            v = row[col]\n",
        "            if isinstance(v, str) and v.strip():\n",
        "                return v.strip()\n",
        "    return \"\"\n",
        "\n",
        "def pick_text(d: Dict[str, Any], keys: List[str]) -> str:\n",
        "    for k in keys:\n",
        "        v = d.get(k)\n",
        "        if isinstance(v, str) and v.strip():\n",
        "            return v.strip()\n",
        "    return \"\"\n",
        "\n",
        "def list_feature_cols(df: pd.DataFrame) -> List[str]:\n",
        "    prefixes = (\"mode_\", \"cause_\", \"place_\", \"myth_\", \"style_\")\n",
        "    cols = [c for c in df.columns if c.startswith(prefixes)]\n",
        "    if not cols:\n",
        "        raise ValueError(\"No feature columns found. Expected prefixes: mode_/cause_/place_/myth_/style_.\")\n",
        "    return cols\n",
        "\n",
        "def ensure_dir(p: str) -> Path:\n",
        "    path = Path(p)\n",
        "    path.mkdir(parents=True, exist_ok=True)\n",
        "    return path\n",
        "\n",
        "def flatten_json_spans(js: Any) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Support both: [chapter{time_series_data:[]}, ...] and {time_series_data:[]} and raw span list.\"\"\"\n",
        "    spans: List[Dict[str, Any]] = []\n",
        "    if isinstance(js, list):\n",
        "        for item in js:\n",
        "            if isinstance(item, dict) and \"time_series_data\" in item:\n",
        "                spans.extend(item.get(\"time_series_data\", []))\n",
        "            elif isinstance(item, dict):\n",
        "                spans.append(item)\n",
        "    elif isinstance(js, dict):\n",
        "        spans = js.get(\"time_series_data\", []) or []\n",
        "    return [s for s in spans if isinstance(s, dict)]\n",
        "\n",
        "def build_span_index(spans: List[Dict[str, Any]]) -> Dict[Tuple[int, int], Dict[str, Any]]:\n",
        "    \"\"\"Index by (episode, global_step).\"\"\"\n",
        "    idx: Dict[Tuple[int, int], Dict[str, Any]] = {}\n",
        "    for s in spans:\n",
        "        ep = s.get(\"episode\")\n",
        "        gs = s.get(\"global_step\")\n",
        "        if ep is None or gs is None:\n",
        "            continue\n",
        "        try:\n",
        "            idx[(int(ep), int(gs))] = s\n",
        "        except Exception:\n",
        "            continue\n",
        "    return idx\n",
        "\n",
        "def infer_keys(df: pd.DataFrame) -> Tuple[str, str]:\n",
        "    \"\"\"Return (episode_col, step_col) for CSV side.\"\"\"\n",
        "    # preferred\n",
        "    if \"episode\" in df.columns and \"global_step\" in df.columns:\n",
        "        return \"episode\", \"global_step\"\n",
        "    # common in your pipeline\n",
        "    if \"chapter\" in df.columns and \"span_id\" in df.columns:\n",
        "        return \"chapter\", \"span_id\"\n",
        "    # last resort\n",
        "    return \"\", \"\"\n",
        "\n",
        "def shorten(s: str, maxlen: int) -> str:\n",
        "    s = (s or \"\").replace(\"\\n\", \" \").strip()\n",
        "    return s if len(s) <= maxlen else s[:maxlen] + \"…\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4585ea5",
      "metadata": {
        "id": "d4585ea5"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 4) Load CSV\n",
        "# ============================================================\n",
        "csv_path = Path(CFG.csv_path)\n",
        "if not csv_path.exists():\n",
        "    raise FileNotFoundError(f\"CSV not found: {csv_path}\")\n",
        "\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "feature_cols = list_feature_cols(df)\n",
        "X = df[feature_cols].to_numpy()\n",
        "\n",
        "episode_col, step_col = infer_keys(df)\n",
        "print(\"rows:\", len(df))\n",
        "print(\"feature_cols:\", len(feature_cols))\n",
        "print(\"csv keys:\", (episode_col, step_col))\n",
        "\n",
        "df.head(3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "219a6de7",
      "metadata": {
        "id": "219a6de7"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 5) Fit GMM + compute posterior entropy / margin\n",
        "# ============================================================\n",
        "Xz = StandardScaler().fit_transform(X)\n",
        "\n",
        "gmm = GaussianMixture(\n",
        "    n_components=CFG.k,\n",
        "    covariance_type=CFG.covariance_type,\n",
        "    random_state=RANDOM_SEED,\n",
        "    reg_covar=CFG.reg_covar,\n",
        "    max_iter=CFG.max_iter,\n",
        "    n_init=CFG.n_init\n",
        ")\n",
        "gmm.fit(Xz)\n",
        "\n",
        "resp = gmm.predict_proba(Xz)  # (N, K)\n",
        "entropy = -(resp * np.log(resp + 1e-12)).sum(axis=1)\n",
        "\n",
        "gmm_state = resp.argmax(axis=1)          # 0-based\n",
        "gmm_state_1based = gmm_state + 1         # 1-based\n",
        "state_change = np.r_[False, gmm_state[1:] != gmm_state[:-1]]\n",
        "\n",
        "# Top2 + margin\n",
        "order = np.argsort(-resp, axis=1)        # desc\n",
        "s1 = order[:, 0]\n",
        "s2 = order[:, 1]\n",
        "p1 = resp[np.arange(len(df)), s1]\n",
        "p2 = resp[np.arange(len(df)), s2]\n",
        "margin = p1 - p2\n",
        "\n",
        "print(\"entropy stats:\",\n",
        "      \"min=\", float(entropy.min()),\n",
        "      \"median=\", float(np.median(entropy)),\n",
        "      \"p95=\", float(np.quantile(entropy, 0.95)),\n",
        "      \"max=\", float(entropy.max()))\n",
        "\n",
        "print(\"state_change rate:\", float(state_change.mean()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "716b6e2e",
      "metadata": {
        "id": "716b6e2e"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 6) boundary candidates: entropy top ALPHA%\n",
        "# ============================================================\n",
        "if not (0 < CFG.alpha <= 1):\n",
        "    raise ValueError(\"alpha must be in (0, 1].\")\n",
        "\n",
        "n_top = int(np.ceil(len(df) * CFG.alpha))\n",
        "top_idx = np.argsort(entropy)[-n_top:][::-1]  # high entropy first\n",
        "\n",
        "print(\"ALPHA:\", CFG.alpha, \"=> n_top:\", n_top)\n",
        "print(\"top entropy:\", float(entropy[top_idx[0]]), \"at row\", int(top_idx[0]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a2cc973",
      "metadata": {
        "id": "2a2cc973"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 7) Load JSON + index spans\n",
        "# ============================================================\n",
        "json_path = Path(CFG.json_path)\n",
        "if not json_path.exists():\n",
        "    raise FileNotFoundError(f\"JSON not found: {json_path}\")\n",
        "\n",
        "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    js = json.load(f)\n",
        "\n",
        "spans = flatten_json_spans(js)\n",
        "span_index = build_span_index(spans)\n",
        "\n",
        "print(\"json spans:\", len(spans))\n",
        "print(\"indexed spans:\", len(span_index))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c1bb6c1",
      "metadata": {
        "id": "0c1bb6c1"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 8) Build boundary_report.csv (evidence prev/next + json span text)\n",
        "# ============================================================\n",
        "rows = []\n",
        "missing_json = 0\n",
        "\n",
        "for i in top_idx:\n",
        "    i = int(i)\n",
        "    # CSV keys\n",
        "    ep = int(df.loc[i, episode_col]) if episode_col else -1\n",
        "    gs = int(df.loc[i, step_col]) if step_col else i\n",
        "\n",
        "    tt = df.loc[i, \"transition_type\"] if \"transition_type\" in df.columns else \"\"\n",
        "\n",
        "    ev = pick_evidence(df.loc[i])\n",
        "    ev_prev = pick_evidence(df.loc[i - 1]) if i - 1 >= 0 else \"\"\n",
        "    ev_next = pick_evidence(df.loc[i + 1]) if i + 1 < len(df) else \"\"\n",
        "\n",
        "    s = span_index.get((ep, gs))\n",
        "    if s is None:\n",
        "        missing_json += 1\n",
        "        s = {}\n",
        "\n",
        "    row = {\n",
        "        \"row_index\": i,\n",
        "        \"episode\": ep,\n",
        "        \"global_step\": gs,\n",
        "        \"transition_type\": tt,\n",
        "\n",
        "        \"entropy\": float(entropy[i]),\n",
        "        \"gmm_state_1based\": int(gmm_state_1based[i]),\n",
        "        \"state_change\": bool(state_change[i]),\n",
        "\n",
        "        \"top1_state_1based\": int(s1[i] + 1),\n",
        "        \"top1_p\": float(p1[i]),\n",
        "        \"top2_state_1based\": int(s2[i] + 1),\n",
        "        \"top2_p\": float(p2[i]),\n",
        "        \"margin\": float(margin[i]),\n",
        "\n",
        "        \"evidence_prev\": ev_prev,\n",
        "        \"evidence\": ev,\n",
        "        \"evidence_next\": ev_next,\n",
        "\n",
        "        \"json_span_text_en\": pick_text(s, [\"span_text_en\", \"text_en\"]),\n",
        "        \"json_span_text_ja\": pick_text(s, [\"span_text_ja\", \"text_ja\"]),\n",
        "        \"json_evidence_en\": pick_text(s, [\"evidence_en\"]),\n",
        "        \"json_evidence_ja\": pick_text(s, [\"evidence_ja\"]),\n",
        "    }\n",
        "\n",
        "    rows.append(row)\n",
        "\n",
        "rep = pd.DataFrame(rows)\n",
        "\n",
        "if CFG.shorten_text:\n",
        "    for c in [\"evidence_prev\", \"evidence\", \"evidence_next\", \"json_span_text_en\", \"json_span_text_ja\"]:\n",
        "        if c in rep.columns:\n",
        "            rep[c] = rep[c].astype(str).map(lambda x: shorten(x, CFG.maxlen))\n",
        "\n",
        "out_dir = ensure_dir(CFG.out_dir)\n",
        "rep_path = out_dir / \"boundary_report.csv\"\n",
        "rep.to_csv(rep_path, index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "print(\"saved:\", rep_path)\n",
        "print(\"json span missing for\", missing_json, \"rows (key mismatch or partial json).\")\n",
        "\n",
        "display(rep.head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f95ccd8",
      "metadata": {
        "id": "6f95ccd8"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 9) turning_points_topK.csv/tsv (paper-ready table)\n",
        "#    - sort by entropy desc, tie-break by margin asc\n",
        "# ============================================================\n",
        "tp = pd.DataFrame({\n",
        "    \"episode\": df[episode_col].astype(int) if episode_col else pd.Series([-1]*len(df)),\n",
        "    \"global_step\": df[step_col].astype(int) if step_col else pd.Series(np.arange(len(df))),\n",
        "\n",
        "    \"transition_type\": df[\"transition_type\"] if \"transition_type\" in df.columns else \"\",\n",
        "    \"entropy\": entropy,\n",
        "    \"gmm_state_1based\": (gmm_state + 1).astype(int),\n",
        "\n",
        "    \"top1_state_1based\": (s1 + 1).astype(int),\n",
        "    \"top1_p\": p1.astype(float),\n",
        "    \"top2_state_1based\": (s2 + 1).astype(int),\n",
        "    \"top2_p\": p2.astype(float),\n",
        "    \"margin\": margin.astype(float),\n",
        "\n",
        "    \"evidence_best\": df.apply(pick_evidence, axis=1),\n",
        "})\n",
        "\n",
        "tp = tp.sort_values([\"entropy\", \"margin\"], ascending=[False, True]).copy()\n",
        "\n",
        "tp_top = tp.head(CFG.turning_points_topk).copy()\n",
        "tp_top.insert(0, \"rank\", np.arange(1, len(tp_top) + 1))\n",
        "\n",
        "tp_csv = out_dir / \"turning_points_top10.csv\"\n",
        "tp_tsv = out_dir / \"turning_points_top10.tsv\"\n",
        "tp_top.to_csv(tp_csv, index=False, encoding=\"utf-8-sig\")\n",
        "tp_top.to_csv(tp_tsv, index=False, sep=\"\\t\", encoding=\"utf-8-sig\")\n",
        "\n",
        "print(\"saved:\", tp_csv)\n",
        "print(\"saved:\", tp_tsv)\n",
        "\n",
        "display(tp_top)\n",
        "\n",
        "print(\"sanity check (should be 1..K):\",\n",
        "      int(tp_top[\"gmm_state_1based\"].min()),\n",
        "      int(tp_top[\"gmm_state_1based\"].max()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53fabd0f",
      "metadata": {
        "id": "53fabd0f"
      },
      "source": [
        "## 追加1： “強い境界”だけ版（entropy上位 AND margin小）\n",
        "\n",
        "- boundaryness（entropy）が高く、かつ top1/top2 の差（margin）が小さい点は  \n",
        "  **「2状態で本当に割れている」**＝解釈しやすい境界になりやすいです。\n",
        "- ここでは **entropyが上位ALPHA%** かつ **margin <= strong_margin_max** を満たす点だけ抽出します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2a94263",
      "metadata": {
        "id": "b2a94263"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 9A) Strong boundaries: (entropy top ALPHA%) AND (margin small)\n",
        "# ============================================================\n",
        "entropy_thr = float(np.quantile(entropy, 1.0 - CFG.alpha))\n",
        "strong_mask = (entropy >= entropy_thr) & (margin <= CFG.strong_margin_max)\n",
        "strong_idx = np.where(strong_mask)[0]\n",
        "\n",
        "print(\"entropy_thr (top ALPHA%):\", entropy_thr)\n",
        "print(\"strong_margin_max:\", CFG.strong_margin_max)\n",
        "print(\"strong boundary count:\", int(len(strong_idx)))\n",
        "\n",
        "# 0件になったら、現実的に使えるように「entropy降順・margin昇順で上位n_top」をフォールバック\n",
        "if len(strong_idx) == 0:\n",
        "    print(\"[fallback] no strong boundaries found -> take top n_top by (entropy desc, margin asc)\")\n",
        "    tmp = pd.DataFrame({\"i\": np.arange(len(df)), \"entropy\": entropy, \"margin\": margin})\n",
        "    tmp = tmp.sort_values([\"entropy\", \"margin\"], ascending=[False, True]).head(n_top)\n",
        "    strong_idx = tmp[\"i\"].astype(int).to_numpy()\n",
        "\n",
        "# Strong boundary report (same schema as boundary_report.csv)\n",
        "rows2 = []\n",
        "missing_json2 = 0\n",
        "for i in strong_idx:\n",
        "    i = int(i)\n",
        "    ep = int(df.loc[i, episode_col]) if episode_col else -1\n",
        "    gs = int(df.loc[i, step_col]) if step_col else i\n",
        "    tt = df.loc[i, \"transition_type\"] if \"transition_type\" in df.columns else \"\"\n",
        "\n",
        "    ev = pick_evidence(df.loc[i])\n",
        "    ev_prev = pick_evidence(df.loc[i - 1]) if i - 1 >= 0 else \"\"\n",
        "    ev_next = pick_evidence(df.loc[i + 1]) if i + 1 < len(df) else \"\"\n",
        "\n",
        "    s = span_index.get((ep, gs))\n",
        "    if s is None:\n",
        "        missing_json2 += 1\n",
        "        s = {}\n",
        "\n",
        "    rows2.append({\n",
        "        \"row_index\": i,\n",
        "        \"episode\": ep,\n",
        "        \"global_step\": gs,\n",
        "        \"transition_type\": tt,\n",
        "        \"entropy\": float(entropy[i]),\n",
        "        \"gmm_state_1based\": int(gmm_state_1based[i]),\n",
        "        \"state_change\": bool(state_change[i]),\n",
        "        \"top1_state_1based\": int(s1[i] + 1),\n",
        "        \"top1_p\": float(p1[i]),\n",
        "        \"top2_state_1based\": int(s2[i] + 1),\n",
        "        \"top2_p\": float(p2[i]),\n",
        "        \"margin\": float(margin[i]),\n",
        "        \"evidence_prev\": ev_prev,\n",
        "        \"evidence\": ev,\n",
        "        \"evidence_next\": ev_next,\n",
        "        \"json_span_text_en\": pick_text(s, [\"span_text_en\", \"text_en\"]),\n",
        "        \"json_span_text_ja\": pick_text(s, [\"span_text_ja\", \"text_ja\"]),\n",
        "        \"json_evidence_en\": pick_text(s, [\"evidence_en\"]),\n",
        "        \"json_evidence_ja\": pick_text(s, [\"evidence_ja\"]),\n",
        "    })\n",
        "\n",
        "strong_rep = pd.DataFrame(rows2)\n",
        "if CFG.shorten_text:\n",
        "    for c in [\"evidence_prev\", \"evidence\", \"evidence_next\", \"json_span_text_en\", \"json_span_text_ja\"]:\n",
        "        if c in strong_rep.columns:\n",
        "            strong_rep[c] = strong_rep[c].astype(str).map(lambda x: shorten(x, CFG.maxlen))\n",
        "\n",
        "strong_rep = strong_rep.sort_values([\"entropy\", \"margin\"], ascending=[False, True]).copy()\n",
        "\n",
        "strong_rep_path = out_dir / \"strong_boundary_report.csv\"\n",
        "strong_rep.to_csv(strong_rep_path, index=False, encoding=\"utf-8-sig\")\n",
        "print(\"saved:\", strong_rep_path, \"| json missing:\", missing_json2)\n",
        "\n",
        "display(strong_rep.head(20))\n",
        "\n",
        "# Paper-ready TopK table for strong boundaries\n",
        "strong_tp = strong_rep.copy()\n",
        "strong_tp = strong_tp.rename(columns={\"evidence\": \"evidence_best\"})\n",
        "strong_tp = strong_tp[[\n",
        "    \"episode\",\"global_step\",\"transition_type\",\n",
        "    \"entropy\",\"gmm_state_1based\",\n",
        "    \"top1_state_1based\",\"top1_p\",\n",
        "    \"top2_state_1based\",\"top2_p\",\n",
        "    \"margin\",\"evidence_best\"\n",
        "]].copy()\n",
        "\n",
        "strong_tp = strong_tp.sort_values([\"entropy\",\"margin\"], ascending=[False, True]).head(CFG.strong_topk).copy()\n",
        "strong_tp.insert(0, \"rank\", np.arange(1, len(strong_tp) + 1))\n",
        "\n",
        "strong_tp_csv = out_dir / \"strong_turning_points_top10.csv\"\n",
        "strong_tp_tsv = out_dir / \"strong_turning_points_top10.tsv\"\n",
        "strong_tp.to_csv(strong_tp_csv, index=False, encoding=\"utf-8-sig\")\n",
        "strong_tp.to_csv(strong_tp_tsv, index=False, sep=\"\\t\", encoding=\"utf-8-sig\")\n",
        "print(\"saved:\", strong_tp_csv)\n",
        "print(\"saved:\", strong_tp_tsv)\n",
        "display(strong_tp)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "483c60c6",
      "metadata": {
        "id": "483c60c6"
      },
      "source": [
        "## 追加2： 遷移点（state_change）優先版（MAPが変わった点のみランキング）\n",
        "\n",
        "- `gmm_state = argmax(resp)` の **MAP状態が前後で変わった点**だけに絞ってランキングします。\n",
        "- ランキングは **entropy降順**、同点なら **margin昇順**（=より混合が強い点を優先）です。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9d33274",
      "metadata": {
        "id": "d9d33274"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 9B) Transition points: state_change == True only\n",
        "# ============================================================\n",
        "trans_idx = np.where(state_change)[0]\n",
        "print(\"transition points (state_change=True):\", int(len(trans_idx)))\n",
        "\n",
        "trans_tp = tp.loc[trans_idx].copy()\n",
        "trans_tp = trans_tp.sort_values([\"entropy\", \"margin\"], ascending=[False, True]).head(CFG.transition_topk).copy()\n",
        "trans_tp.insert(0, \"rank\", np.arange(1, len(trans_tp) + 1))\n",
        "\n",
        "trans_csv = out_dir / \"transition_points_top10.csv\"\n",
        "trans_tsv = out_dir / \"transition_points_top10.tsv\"\n",
        "trans_tp.to_csv(trans_csv, index=False, encoding=\"utf-8-sig\")\n",
        "trans_tp.to_csv(trans_tsv, index=False, sep=\"\\t\", encoding=\"utf-8-sig\")\n",
        "\n",
        "print(\"saved:\", trans_csv)\n",
        "print(\"saved:\", trans_tsv)\n",
        "display(trans_tp)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1df82737",
      "metadata": {
        "id": "1df82737"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 10) Quick sanity summary (optional)\n",
        "# ============================================================\n",
        "summary = {\n",
        "    \"N\": int(len(df)),\n",
        "    \"K\": int(CFG.k),\n",
        "    \"alpha\": float(CFG.alpha),\n",
        "    \"boundary_n\": int(len(rep)),\n",
        "    \"entropy_max\": float(entropy.max()),\n",
        "    \"entropy_p95\": float(np.quantile(entropy, 0.95)),\n",
        "    \"entropy_median\": float(np.median(entropy)),\n",
        "    \"state_change_rate\": float(state_change.mean()),\n",
        "    \"boundary_state_change_rate\": float(rep[\"state_change\"].mean()),\n",
        "    \"json_missing_rate_in_boundary\": float(missing_json / max(1, len(rep))),\n",
        "}\n",
        "pd.DataFrame([summary])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0a89567",
      "metadata": {
        "id": "e0a89567"
      },
      "source": [
        "## 出力物\n",
        "\n",
        "- `out/boundary_report.csv`  \n",
        "  entropy上位(ALPHA%)の候補。前後evidence、top2 states、margin、JSON側span_text付き。\n",
        "\n",
        "- `out/turning_points_top10.csv` / `out/turning_points_top10.tsv`  \n",
        "  本文に貼る用の TopK 表。\n",
        "\n",
        "## GitHubに上げるときのおすすめ\n",
        "\n",
        "- ノートブック名（例）: `01_ulysses_boundary_report.ipynb`\n",
        "- 依存を減らすため、外部自作モジュール無しで完結させています。\n",
        "- データファイルは `.gitignore` で除外し、READMEに「Colabでアップロードして動かす」手順を書くのが安全です。\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}